{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydeS6SHN6oDm"
   },
   "outputs": [],
   "source": [
    "# This program is used to train the Gate Recurrent Neural Network \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import glob,os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor    \n",
    "torch.manual_seed(125)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the recording data from the folder\n",
    "base_path = ' ' # enter your local path\n",
    "for k in range (65,92):\n",
    "    #alphabet = ord(alphabet)-97\n",
    "    a = np.load(base_path+'/'+chr(k)+chr(k)+'.npy')\n",
    "    for ax in range(3):\n",
    "        for i in range(5,100): # how many times you record\n",
    "            for j in range(400): # time points in one trial\n",
    "                temp = j\n",
    "                while (a[i][ax][j]<-60): # arduino sometimes pass 0 to the computer, as a result, use previous data to replace 0\n",
    "                    temp = j-1\n",
    "                    a[i][ax][j] = a[i][ax][temp]\n",
    "    for ax in range(3): # min max normalization\n",
    "        for i in range(5,100):\n",
    "            min = np.mean(a[i][ax])\n",
    "            max = np.max(a[i][ax])\n",
    "            a[i][ax] = (a[i][ax]-min )/(max-min)\n",
    "    np.save(base_path+'/'+chr(k)+'_r',a)\n",
    "#plot x y z data\n",
    "for k in range(len(thre)):\n",
    "    for i in range(15,45):\n",
    "        plt.plot(a[i][0][thre[i]:thre[i]+195])\n",
    "plt.show()\n",
    "for k in range(len(thre)):\n",
    "    for i in range(15,45):\n",
    "        plt.plot(a[i][1][thre[i]:thre[i]+195])\n",
    "plt.show()\n",
    "for k in range(len(thre)):\n",
    "    for i in range(15,45):\n",
    "        plt.plot(a[i][2][thre[i]:thre[i]+195])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0e71T7G6oDo"
   },
   "outputs": [],
   "source": [
    "#read train dataset\n",
    "train_dir = base_path\n",
    "train_names = glob.glob(os.path.join(train_dir, '*') )\n",
    "train_numpy = []\n",
    "train_name = []\n",
    "for image_path in train_names:\n",
    "    #print(image_path)\n",
    "    alphabet = image_path[-7]\n",
    "    alphabet = ord(alphabet)-65\n",
    "    alp_arr = [alphabet] * 95\n",
    "    im = np.load(image_path)\n",
    "    train_numpy.append(im[5:])\n",
    "    train_name.append(alp_arr)\n",
    "    print(image_path[-7],alphabet,len(train_numpy))\n",
    "train_numpy = np.array(train_numpy).reshape(([-1,4,400])).astype('float')\n",
    "train_numpy = np.transpose(train_numpy, (0, 2, 1))\n",
    "train_name = np.array(train_name).reshape((-1))\n",
    "np.save('train',train_numpy)\n",
    "np.save('train_alphabet',train_name)\n",
    "print(train_numpy.shape)\n",
    "print(train_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjllBZs56oDp"
   },
   "outputs": [],
   "source": [
    "#Min max normalizaion\n",
    "for i in range(len(train_numpy)):\n",
    "    for k in range(3):\n",
    "        max_num = np.max(train_numpy[i,:,k])\n",
    "        min_num = np.min(train_numpy[i,:,k])\n",
    "        if((max_num - min_num)!= 0):\n",
    "            train_numpy[i,:,k] = (train_numpy[i,:,k]-min_num)/(max_num - min_num)\n",
    "        else:\n",
    "            train_numpy[i,:,k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f0ngWl36oDq"
   },
   "outputs": [],
   "source": [
    "#Pytorch dataset\n",
    "class CustomTensorDataset(TensorDataset):\n",
    "    \"\"\"TensorDataset with support of transforms.\n",
    "    \"\"\"\n",
    "    def __init__(self, tensors,alphabet):\n",
    "        self.tensors = tensors\n",
    "        self.alphabet = alphabet\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.tensors[index]\n",
    "        alph = self.alphabet[index]\n",
    "        return x,alph.float() \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMqCDWDJ6oDr"
   },
   "outputs": [],
   "source": [
    "#Dataloader\n",
    "batch_size = 64\n",
    "data = torch.from_numpy(train_numpy)\n",
    "name = torch.from_numpy(train_name)\n",
    "full_dataset = CustomTensorDataset(data,name)\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)\n",
    "test_sampler = RandomSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4REhQifv6oDt"
   },
   "source": [
    "## GRU cell implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_cOubpK6oDt"
   },
   "outputs": [],
   "source": [
    "class GRUCell(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    An implementation of GRUCell.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super(GRUCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        self.x2h = nn.Linear(input_size, 3 * hidden_size, bias=bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 3 * hidden_size, bias=bias)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        std = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for w in self.parameters():\n",
    "            w.data.uniform_(-std, std)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x = x.view(-1, x.size(1))\n",
    "        \n",
    "        gate_x = self.x2h(x) \n",
    "        gate_h = self.h2h(hidden)\n",
    "        \n",
    "        gate_x = gate_x.squeeze()\n",
    "        gate_h = gate_h.squeeze()\n",
    "        if(len(gate_x.size())<=1):\n",
    "            gate_x = gate_x.unsqueeze(0)\n",
    "            gate_h = gate_h.unsqueeze(0)             \n",
    "        i_r, i_i, i_n = gate_x.chunk(3, 1)\n",
    "        h_r, h_i, h_n = gate_h.chunk(3, 1)\n",
    "        \n",
    "        \n",
    "        resetgate = F.sigmoid(i_r + h_r)\n",
    "        inputgate = F.sigmoid(i_i + h_i)\n",
    "        newgate = F.tanh(i_n + (resetgate * h_n))\n",
    "        \n",
    "        hy = newgate + inputgate * (hidden - newgate)\n",
    "        \n",
    "        \n",
    "        return hy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3cv_7Im6oDv"
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, bias=True):\n",
    "        super(GRUModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "         \n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "         \n",
    "       \n",
    "        self.gru_cell = GRUCell(input_dim, hidden_dim, layer_dim)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "     \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.layer_dim, x.size(0), self.hidden_dim))\n",
    "         \n",
    "       \n",
    "        outs = []\n",
    "        \n",
    "        hn = h0[0,:,:]\n",
    "        \n",
    "        for seq in range(x.size(1)):\n",
    "            hn = self.gru_cell(x[:,seq,:], hn) \n",
    "            outs.append(hn)\n",
    "            \n",
    "\n",
    "        out = outs[-1].squeeze()\n",
    "        \n",
    "        out = self.fc(out) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PC5qHtnk6oDw"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_dim = 4\n",
    "hidden_dim = 128\n",
    "layer_dim = 1  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 26\n",
    "model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "     \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9GxNug_6oDw"
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "summary(model, (400, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Xy_KmuH6oDx",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 400 #time points of each alphabet\n",
    "batch_size = 32\n",
    "num_epochs = 6000 \n",
    "loss_list = []\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(images, labels) in enumerate(train_dataloader):\n",
    "          \n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).cuda().float())\n",
    "            labels = Variable(labels.cuda())\n",
    "        else:\n",
    "            images = Variable(images.view(-1, seq_dim, input_dim).float() )\n",
    "            labels = Variable(labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.to(torch.int64))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            loss.cuda()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        iter += 1\n",
    "         \n",
    "        if iter % 1 == 0:      \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for (images, labels) in test_dataloader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = Variable(images.view(-1, seq_dim, input_dim).cuda().float())\n",
    "                else:\n",
    "                    images = Variable(images.view(-1 , seq_dim, input_dim).float())\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "             \n",
    "            accuracy = 100 * correct / total\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZUZImzs6oDy"
   },
   "outputs": [],
   "source": [
    "torch.save(model, 'all_final.pt')\n",
    "torch.save(model.state_dict(), 'all_weight.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "lstm_gru-all.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
